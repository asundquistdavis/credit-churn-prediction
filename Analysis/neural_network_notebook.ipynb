{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN Dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "## Other Dependencies\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot style selection - applies style to all matplotlib plots \n",
    "plt.style.use(['default','seaborn-whitegrid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77e34f",
   "metadata": {},
   "source": [
    "## Initial Data import and pre-processing for Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataset csv\n",
    "\n",
    "churn_df = pd.read_csv('../Resources/clean_churn_db.csv')\n",
    "attrition_df = pd.read_csv('../Resources/BankChurners.csv', usecols=['Attrition_Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The amount of Attrited Customers/Existing Customers in the dataset is 1628/8500 or {round(1627/8500 * 100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61baff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in churn_df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd80693",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = churn_df.drop(columns=churn_df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71db822",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define feature values X\n",
    "X = churn_df.values\n",
    "\n",
    "## Define target values y\n",
    "y_df = attrition_df.replace({'Existing Customer':0, 'Attrited Customer':1}).copy()\n",
    "y = y_df['Attrition_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.read_csv('../Resources/X.csv')\n",
    "# y = pd.read_csv('../Resources/y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instance Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "## Fit Scaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "## Scale Data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_scaled[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b2c819",
   "metadata": {},
   "source": [
    "## Initial Modelling Attempt\n",
    "Using all the availible features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layering, beginning with 1 hidden layer\n",
    "input_features = len(X_train_scaled[0])\n",
    "hidden_layer_1 = 25\n",
    "outputs = 1\n",
    "\n",
    "nn_init = tf.keras.models.Sequential(name='initial')\n",
    "\n",
    "## First Hidden Layer + Input\n",
    "nn_init.add(tf.keras.layers.Dense(units = hidden_layer_1, input_dim = input_features, activation = 'relu'))\n",
    "\n",
    "##Output Layer\n",
    "nn_init.add(tf.keras.layers.Dense(units = outputs, activation='sigmoid'))\n",
    "\n",
    "nn_init.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ee463",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile and fit\n",
    "nn_init.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',])\n",
    "initial_model = nn_init.fit(X_train_scaled, y_train, validation_data = (X_test_scaled, y_test) , epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss, initial_accuracy = nn_init.evaluate(X_test_scaled, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4974a",
   "metadata": {},
   "source": [
    "#### Is this accuracy true?\n",
    "We can verify through validation data. Lets look at the plots for accuracy and loss for both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba52cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1, initial = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "initial_1 = initial[0].plot(initial_model.history['accuracy'],label='Accuracy', color=\"navy\")\n",
    "initial_2 = initial[0].plot(initial_model.history['val_accuracy'],label='Validation', color=\"darkorange\")\n",
    "initial[0].legend(loc='lower right')\n",
    "initial[0].set_xlim([0,100])\n",
    "initial[0].set_ylim([.7,1])\n",
    "initial[0].text(40,.71,f'Evaluated Accuracy: {round(initial_accuracy,4)}',fontsize=12)\n",
    "\n",
    "initial_3 = initial[1].plot(initial_model.history['loss'],label='Loss', color=\"navy\")\n",
    "initial_4 = initial[1].plot(initial_model.history['val_loss'],label='Validation', color=\"darkorange\")\n",
    "initial[1].legend(loc='upper right')\n",
    "initial[1].set_xlim([0,100])\n",
    "initial[1].set_ylim([0,.5])\n",
    "initial[1].text(42,.47,f'Evaluated Loss: {round(initial_loss,4)}',fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig(\"../../static/assets//initial_model.png\",facecolor='white')\n",
    "fig1.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69dfd80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Save to CSV for replication as need be.\n",
    "initial_model_df= pd.DataFrame({\n",
    "    'Loss': initial_model.history['loss'],\n",
    "    'Validation Loss': initial_model.history['val_loss'],\n",
    "    'Accuracy': initial_model.history['accuracy'],\n",
    "    'Validation Accuracy': initial_model.history['val_accuracy'],\n",
    "    \n",
    "})\n",
    "\n",
    "initial_model_df.to_csv('../../static/assets//initial_model.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8ec15",
   "metadata": {},
   "source": [
    "Model seems to be overfitted, adjustments to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea707d9",
   "metadata": {},
   "source": [
    "## Adjusting Model using L2 regularization\n",
    "\n",
    "L2 regularization is a standard response to overfitted models. This kind of regularization adds a penalty to the weight values of the nodes on the layer it is activated in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65563f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regularizer import\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f34ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layering, beginning with 1 hidden layer\n",
    "input_features = len(X_train_scaled[0])\n",
    "hidden_layer_1 = 25\n",
    "outputs = 1\n",
    "\n",
    "nn_l2 = tf.keras.models.Sequential(name='l2_reg')\n",
    "\n",
    "## First Hidden Layer + Input. Add regularizer. \n",
    "nn_l2.add(tf.keras.layers.Dense(units = hidden_layer_1, input_dim = input_features, activation = 'relu',\n",
    "                            kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "##Output Layer\n",
    "nn_l2.add(tf.keras.layers.Dense(units = outputs, activation='sigmoid'))\n",
    "\n",
    "nn_l2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_l2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',])\n",
    "l2_reg_model = nn_l2.fit(X_train_scaled, y_train, validation_data = (X_test_scaled, y_test) , epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss, l2_accuracy = nn_l2.evaluate(X_test_scaled, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61fc5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Check for performance/validation. \n",
    "fig2, L2 = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "L2[0].plot(l2_reg_model.history['accuracy'], label=\"Accuracy\", color=\"navy\")\n",
    "L2[0].plot(l2_reg_model.history['val_accuracy'], label=\"Validation\", color=\"darkorange\")\n",
    "L2[0].legend(loc='lower right')\n",
    "L2[0].set_xlim([0,100])\n",
    "L2[0].set_ylim([.7,1])\n",
    "L2[0].text(40,.71,f'Evaluated Accuracy: {round(l2_accuracy,4)}',fontsize=12)\n",
    "\n",
    "\n",
    "L2[1].plot(l2_reg_model.history['loss'], label=\"Loss\", color=\"navy\")\n",
    "L2[1].plot(l2_reg_model.history['val_loss'], label=\"Validation\", color=\"darkorange\")\n",
    "L2[1].legend(loc='upper right')\n",
    "L2[1].set_xlim([0,100])\n",
    "L2[1].set_ylim([0,.5])\n",
    "L2[1].text(42,.47,f'Evaluated Loss: {round(l2_loss,4)}',fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig(\"../../static/assets//L2_model.png\",facecolor='white')\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_model_df= pd.DataFrame({\n",
    "    'Loss': l2_reg_model.history['loss'],\n",
    "    'Validation Loss': l2_reg_model.history['val_loss'],\n",
    "    'Accuracy': l2_reg_model.history['accuracy'],\n",
    "    'Validation Accuracy': l2_reg_model.history['val_accuracy'],\n",
    "    \n",
    "})\n",
    "\n",
    "L2_model_df.to_csv('../Outputs//neural_network/L2_model.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looks good at first, but the model still finds out a pattern early and does not adapt to new information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59f2aa",
   "metadata": {},
   "source": [
    "## Adjusting model using Dropout\n",
    "\n",
    "Dropout is another standard method in response to overfitting. Drops nodes by probability to decrease any given node from correct the mistakes of other nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layering, beginning with 1 hidden layer\n",
    "input_features = len(X_train_scaled[0])\n",
    "hidden_layer_1 = 25\n",
    "outputs = 1\n",
    "\n",
    "nn_dropout = tf.keras.models.Sequential(name='dropout')\n",
    "\n",
    "## First Hidden Layer + Input\n",
    "\n",
    "nn_dropout.add(tf.keras.layers.Dense(units = hidden_layer_1, input_dim = input_features, activation = 'relu'))\n",
    "\n",
    "## Add dropout\n",
    "nn_dropout.add(tf.keras.layers.Dropout(.2))\n",
    "\n",
    "##Output Layer\n",
    "nn_dropout.add(tf.keras.layers.Dense(units = outputs, activation='sigmoid'))\n",
    "\n",
    "\n",
    "nn_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dropout.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',])\n",
    "dropout_model = nn_dropout.fit(X_train_scaled, y_train, validation_data = (X_test_scaled, y_test) , epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176eae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_loss, dropout_accuracy = nn_dropout.evaluate(X_test_scaled, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86514d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, dropout = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "dropout[0].plot(dropout_model.history['accuracy'], label='Accuracy', color=\"navy\")\n",
    "dropout[0].plot(dropout_model.history['val_accuracy'], label='Validation', color=\"darkorange\")\n",
    "dropout[0].legend(loc='lower right')\n",
    "dropout[0].set_xlim([0,100])\n",
    "dropout[0].set_ylim([.7,1])\n",
    "dropout[0].text(40,.71,f'Evaluated Accuracy: {round(dropout_accuracy,4)}',fontsize=12)\n",
    "\n",
    "dropout[1].plot(dropout_model.history['loss'], label='Loss', color=\"navy\")\n",
    "dropout[1].plot(dropout_model.history['val_loss'], label='Validation', color=\"darkorange\")\n",
    "dropout[1].legend(loc='upper right')\n",
    "dropout[1].set_xlim([0,100])\n",
    "dropout[1].set_ylim([0,.5])\n",
    "dropout[1].text(42,.47,f'Evaluated Loss: {round(dropout_loss,4)}',fontsize=12)\n",
    "\n",
    "plt.savefig(\"../../static/assets//dropout_model.png\",facecolor='white')\n",
    "fig3.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5295a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_model_df= pd.DataFrame({\n",
    "    'Loss': dropout_model.history['loss'],\n",
    "    'Validation Loss': dropout_model.history['val_loss'],\n",
    "    'Accuracy': dropout_model.history['accuracy'],\n",
    "    'Validation Accuracy': dropout_model.history['val_accuracy'],\n",
    "    \n",
    "})\n",
    "\n",
    "dropout_model_df.to_csv('../Outputs//neural_network/dropout_model.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9fe26",
   "metadata": {},
   "source": [
    "Looking better as well. The next step will combine both L2 and dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c022",
   "metadata": {},
   "source": [
    "## Using both L2 and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Layering, beginning with 1 hidden layer\n",
    "input_features = len(X_train_scaled[0])\n",
    "hidden_layer_1 = 25\n",
    "outputs = 1\n",
    "\n",
    "nn_l2_dropout = tf.keras.models.Sequential(name='dropout_and_l2')\n",
    "\n",
    "## First Hidden Layer + Input + regularizer.\n",
    "nn_l2_dropout.add(tf.keras.layers.Dense(units = hidden_layer_1, input_dim = input_features, activation = 'relu',\n",
    "                             kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "## Add dropout\n",
    "nn_l2_dropout.add(tf.keras.layers.Dropout(.2))\n",
    "\n",
    "\n",
    "##Output Layer\n",
    "nn_l2_dropout.add(tf.keras.layers.Dense(units = outputs, activation='sigmoid'))\n",
    "\n",
    "\n",
    "nn_l2_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60970669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_l2_dropout.compile(\n",
    "    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', \n",
    "    tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "dropout_l2_model = nn_l2_dropout.fit(X_train_scaled, y_train, validation_data = (X_test_scaled, y_test) , epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ad0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropl2_loss, dropl2_accuracy, dropl2_prec, dropl2_recall = nn_l2_dropout.evaluate(X_test_scaled, y_test, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72635d99",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig4, L2_drop = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "L2_drop[0].plot(dropout_l2_model.history['accuracy'], label='Accuracy', c='orange')\n",
    "L2_drop[0].plot(dropout_l2_model.history['val_accuracy'], label='Validation', color=\"navy\")\n",
    "L2_drop[0].legend()\n",
    "L2_drop[0].legend(loc='lower right')\n",
    "L2_drop[0].set_xlim([0,100])\n",
    "L2_drop[0].set_ylim([.7,1])\n",
    "L2_drop[0].text(40,.71,f'Evaluated Accuracy: {round(dropl2_accuracy,4)}',fontsize=12)\n",
    "\n",
    "L2_drop[1].plot(dropout_l2_model.history['loss'], label='Loss', c='orange')\n",
    "L2_drop[1].plot(dropout_l2_model.history['val_loss'], label='Validation', color=\"navy\")\n",
    "L2_drop[1].legend()\n",
    "L2_drop[1].legend(loc='upper right')\n",
    "L2_drop[1].set_xlim([0,100])\n",
    "L2_drop[1].set_ylim([0,.5])\n",
    "L2_drop[1].text(42,.47,f'Evaluated Loss: {round(dropl2_loss,4)}',fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig(\"../../static/assets//dropout_L2_model.png\",facecolor='white')\n",
    "fig4.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a38217",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_l2_df= pd.DataFrame({\n",
    "    'Loss': dropout_l2_model.history['loss'],\n",
    "    'Validation Loss': dropout_l2_model.history['val_loss'],\n",
    "    'Accuracy': dropout_l2_model.history['accuracy'],\n",
    "    'Validation Accuracy': dropout_l2_model.history['val_accuracy'],\n",
    "    \n",
    "})\n",
    "\n",
    "dropout_l2_df.to_csv('../Outputs//neural_network/dropout_l2_history.csv',index=False)\n",
    "dropout_l2_df.to_hdf('../Outputs//neural_network/dropout_l2_model.h5',key='dl2_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28d716",
   "metadata": {},
   "source": [
    "Good convergence with validation and training. The problem of overfitting has been resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad4844",
   "metadata": {},
   "source": [
    "## Continued Visualization\n",
    "\n",
    "In order to make the best judgements about the model, additional visualizations should be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f8b51",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a47703",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = nn_l2_dropout.predict(X_test_scaled) > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668786ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355fb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tf.math.confusion_matrix(y_test,y_pred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c610c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = pd.DataFrame(matrix)\n",
    "matrix_df = matrix_df.rename(columns={0:\"Positive_r\",1:\"Negative_r\"}, index={0:\"Positive_c\",1:\"Negative_c\"})\n",
    "matrix_df.to_csv(\"../Outputs//neural_network/con_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334a083",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(nn_l2_dropout, to_file='../Outputs//neural_network/dropout_l2_arch.png', show_shapes=True, \n",
    "                          show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34330953",
   "metadata": {},
   "source": [
    "### ROC curve and AOC-ROC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdbc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import methods for ROC curve, AUC and AUC score.\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "## Utilize method from Tensorflow Keras documentation\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(37):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_nn)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9da5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred_nn.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f218eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(\n",
    "    fpr[2],\n",
    "    tpr[2],\n",
    "    color=\"darkorange\",\n",
    "    lw=2,\n",
    "    label=\"ROC curve (area = %0.4f)\" % roc_auc[2],\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig(\"../../static/assets//nn_ROC.png\",facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ef1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export false positive rate, true positive rate, and coresponding auc value for use as needed \n",
    "nn_roc = pd.DataFrame({\"fpr\":fpr[2],'tpr':tpr[2],'AUC':roc_auc[2]})\n",
    "nn_roc.to_csv('../Outputs//neural_network/nn_roc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489eff2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
